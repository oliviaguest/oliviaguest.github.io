---
published: true
layout: post
---

### Introduction

In psychology and specifically in social psychology, replication or lack thereof has been a widely discussed issue, especially since it has been dubbed a "[crisis](http://pps.sagepub.com/content/9/1/59.short)" [by](http://pps.sagepub.com/content/7/6/531.short) [some](http://pps.sagepub.com/content/9/3/305.short). [Studies](http://econtent.hogrefe.com/toc/zsp/45/3) [looking](http://www.sciencemag.org/content/349/6251/aac4716) at the reproducibility of published research receive [academic](http://www.nytimes.com/2015/09/01/opinion/psychology-is-not-in-crisis.html) and [media](http://www.theguardian.com/commentisfree/2015/aug/28/psychology-experiments-failing-replication-test-findings-science) attention, demonstrating how important failures to replicate are perceived to be not just because they are taken ([often](http://psycnet.apa.org/psycinfo/2015-15456-001/) [mistakenly](http://journal.frontiersin.org/article/10.3389/fpsyg.2014.01276/abstract)) as [falsifying published research](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124) but also because they are a relative novelty. This is because it continues to be the case that [publication](http://www.sciencemag.org/content/345/6203/1502) [bias](http://www.nature.com/news/social-sciences-suffer-from-severe-publication-bias-1.15787), the [file-drawer effect](http://www.psychfiledrawer.org/scholarly_articles.php), [accusations](http://www.theguardian.com/science/head-quarters/2014/may/28/psychology-replication-drive-methods-bullying) [of](http://www.scribd.com/doc/225285909/Kahneman-Commentary) [bullying](http://www.sciencemag.org/content/344/6186/788.full) and [personal attacks](http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/#.VgQmiJfsY_u), and other [academic](http://pss.sagepub.com/content/22/11/1359) [cultural](http://www.sciencemag.org/content/348/6242/1422.summary) [norms](http://pps.sagepub.com/content/7/6/608.long) dissuade researchers from writing [journal articles about experiments that "do not work"](http://pps.sagepub.com/content/7/6/537.full.pdf+html).

In the sub-domain of [cognitive computational modelling](http://psych.stanford.edu/~jlm/papers/McClelland09PlaceOfModelingtopiCS.pdf), replication aka reimplementation is [not a very](https://scholar.google.co.uk/scholar?q=%22computational+modelling%22+replication) [hot topic](https://scholar.google.co.uk/scholar?q=%22cognitive+modelling%22+replication) in neither normal (i.e., as an integral part of the scientific process) nor exaggerated (i.e., crisis) form. However, there is a newly created journal, [ReScience](http://rescience.github.io/), dedicated to replicating all types of computational models; a [paper](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000456) and a [book chapter](http://link.springer.com/chapter/10.1007%2F978-1-4614-1424-7_4) focussing on non-cognitive neuroscientific models, which make some very important points that apply equally to cognitive models; and furthermore computational modellers outside the cognitive sciences have [discussed](http://www.sciencemag.org/content/334/6060/1226.long) [replication](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285) albeit in a way that does not subserve identical scientific aims. The reasons for this perceived lack of interest within cognitive computational modelling are mainly because:

a. Models are often used as-is. As opposed to programmed from scratch, they are inherited from other labs. This is a sensible choice for many.

b. There is confidence in specific implementations of models. Researchers assume that few or no bugs are present in published work. They also take for granted that current implementations represent the full space of possible models given the [specification](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7091763); that models generated from the published specification will only differ superficially; that the specification is sufficient to generate useful implementations. The former is sensible for many to assume. Especially if a codebase is openly accessible online it is [less likely to be have very serious bugs](http://www.nature.com/news/2003/030623/full/news030623-6.html). The latter is less easy to verify without examining the parameter space of the model and without replicating based on the provided specification.

c. There is a lack of theoretical consensus as to what a model is. Some see models and implementations of models (simulations) as identical entities and some [do not](http://philsci-archive.pitt.edu/2767/1/SimsAreNotModelsRD7.pdf). This further complicates the rhetoric around replication efforts by defining a replication as a completely new model within a theoretical framework as opposed to a reimplementation of a pre-existing model.

d. Cognitive modelling is an inter- and trans-disciplinary sub-community. Members from engineering, physics, computer science, linguistics, as well as psychology all come together to work on simulations of cognition - by definition a small intersection of the scientific community. Psychology and cognitive science students and researchers are less likely to be involved in modelling than they are to engaged with empirical work and other types of theory evaluation partly because programming is not conclusively and rigorously taught (but hopefully change is afoot!) to neither under- nor post-graduate students.

On the other hand, replication in this context can have a different broader meaning that applies to theories as well as models. That is: does the empirical evidence that supports the model also replicate? Both replicating the supporting evidence for a model and the model's behaviours, [predictions, explanatory mechanisms](http://iccm-conference.org/2013-proceedings/papers/0074/paper0074.pdf), etc., themselves are important for building upon a specific framework. So in much the same way that, e.g., imaging and behavioural data must both agree in support of a specific theory, the internals of a model (the implementation) and its grounding in the real world (the participant data) must be congruent. The behaviour of the model must match the behaviour of the participants. If participants are shown to be doing something different to those described in the modelling article but the model nonetheless replicates then that is as relevant to the model and overarching theory as the model itself failing to replicate. Although of course [theory or model fit](http://www.ncbi.nlm.nih.gov/pubmed/10789200) [is not the be-all and end-all](http://www.sciencedirect.com/science/article/pii/S1364661302019642), but lack thereof is troubling.

### Why attempt to replicate?

Firstly, it must be noted there are two types of replication: direct and conceptual. Direct replication is one that is meant to be as faithful a copy as the original authors' specification allows. Conceptual replication is less faithful to the original published model but still falls within the general theory or account. One possibility for the latter is to radically modify some aspect of the modelling strategy (e.g., use a completely different type of learning algorithm) that is nonetheless not clearly stated as integral to the account's success. Replication without a qualifier, as used so far, implies replication as direct as possible. Further [contrasts between the two](http://neurochambers.blogspot.co.uk/2012/03/you-cant-replicate-concept.html) will not be made here; although [some](https://cedarsdigest.wordpress.com/2012/03/21/put-your-head-up-to-the-meta-a-peer-reviews-post-post-publication-peer-review-a-bargh-full-of-links/) deem "conceptual" to be a [weasel word](https://en.wikipedia.org/wiki/Weasel_word).

Now we can return to the motivation for replicating computational models: Scientists do not rewrite a new word processor every time they want to make sure their journal articles look good, so why reprogram a pre-existing model? They certainly do not, but the companies that produce software like that do often rewrite their whole codebase. They are cunning and do so often without us realising because they carefully [replicate the GUI parts](https://github.com/fluentribbon/Fluent.Ribbon/blob/master/README.md) they want to keep. They need to do so for the normal reasons all software needs rewriting or updating: bugs are found and patched, hardware changes dramatically over time, more efficient implementations are discovered (e.g., [bubble sort](https://en.wikipedia.org/wiki/Bubble_sort) vs [heapsort](https://en.wikipedia.org/wiki/Heapsort) vs [quicksort](https://en.wikipedia.org/wiki/Quicksort)), and so on.

So in addition to some of the computer engineering reasons for reimplementing models there are some more pertinent pedagogical and scientific ones. The same sorts of reasons that require replication as in experimental psychology more broadly apply to computational modelling efforts within the cognitive sciences. Here are the main objectives of and motivations for replication in the form of questions and answers:

0. _Does it even work?_
Most importantly, reimplementing a model allows us to evaluate if the model really is accounting for the phenomenon it claims to, using the mechanisms it purports to, and appealing to the properties of those mechanisms in an appropriate way. This question can also be posed more verbosely: Given the specification, the methods section, can the results be recreated? This is exactly same reason why one might re-run any experiment, especially before embarking on further research within the sub-area of psychological science one is interested in. In other words, just as one would not merely take the data collected by other researchers as absolute evidence their experiment works, one would not take the output of a pre-existing implementation as evidence the model works. Of course, vice versa is also true: one would not and should not take as evidence an effect does not exist merely because it did not replicate once.

[![](http://4.bp.blogspot.com/-qZWHKmmaIJY/VgleBxsNFrI/AAAAAAAAESA/p5BnLEy--c8/s320/index.png)](http://4.bp.blogspot.com/-qZWHKmmaIJY/VgleBxsNFrI/AAAAAAAAESA/p5BnLEy--c8/s1600/index.png)
Not all bugs are this obvious! A [legendary bug](http://chsi.harvard.edu/markone/language.html) discovered by [Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper). 

1. _Are there any (important) bugs?_
Mistakes happen - both logical and conceptual bugs are introduced into a piece of software [even by the best of coders](http://www.fastcompany.com/28121/they-write-right-stuff). As such, the important bugs need to be discovered one way or another. In computational modelling modellers rarely run proper testing (e.g., [unit testing](https://en.wikipedia.org/wiki/Unit_testing)) like they (sometimes try to) do in software engineering. Replication is a brute force, but often quicker/easier, way to understand (see question 3), and uncover the potential bugs in, another scientists' ([spaghetti](https://en.wikipedia.org/wiki/Spaghetti_code)) code. So while similar mistakes happen with testing participants and with analysing their data, bugs in computational modelling is a much more opaque problem to tackle than, e.g., bugs in code to present stimuli to participants.

[![](https://upload.wikimedia.org/wikipedia/commons/0/0a/Animation._1200_iterations_of_the_'Rule_110'_Automata.gif)](https://upload.wikimedia.org/wikipedia/commons/0/0a/Animation._1200_iterations_of_the_%27Rule_110%27_Automata.gif)
Choose an appropriate programming language. Just because [rule 110](https://en.wikipedia.org/wiki/Rule_110) [is Turing complete](http://www.complex-systems.com/pdf/15-1-1.pdf), does not mean we should write models using cellular automata!

2. _Can a computer/person still execute/read it?_
Newer operating systems, programming languages, and even hardware sometimes mean that models (even from less than a decade ago) will not (easily) run on newer machines. Think about all the (often minor) compatibility problems one encounters merely from one version of [Python](https://wiki.python.org/moin/Python2orPython3) or [Matlab](http://uk.mathworks.com/products/matlab/whatsnew.html) to another only within the span of a couple of years. Modellers need to be able to run models easily on modern machines and they need to be written in appropriate languages. If not because we actually prefer [Python over LISP,](http://norvig.com/python-lisp.html) then because we want our code to be readable, executable and understandable by those who have been taught [Python or Matlab](http://www.pyzo.org/python_vs_matlab.html) (i.e., the subset of students in psychology who have programming experience). It is inevitable that models will be, given time, practically unusable simply because their implementation has been rendered obsolete. So old code must be translated or better still [rewritten from scratch into new code](http://journal.frontiersin.org/article/10.3389/fncom.2015.00030/full). This problem exists for all scientists, who depend on using as up-to-date hardware and software as their funding allows. Nobody wants to be using Windows XP or Vista any more, however much they might be comfortable and familiar with it.

[![](https://upload.wikimedia.org/wikipedia/commons/5/58/FortranCardPROJ039.agr.jpg)](https://upload.wikimedia.org/wikipedia/commons/5/58/FortranCardPROJ039.agr.jpg)
Implementations become obsolete relatively quickly. While Fortran is [still used](https://www.quora.com/Is-FORTRAN-still-being-used-today-If-yes-what-do-people-use-it-for), good luck trying to run this [program](https://en.wikipedia.org/wiki/Punched_card) on a laptop!

3. _Can a person understand it?_
Even the best coder if they are agnostic to cognitive science will struggle to understand what a model is simulating. Because even though they will unarguably understand the code itself, they will not be familiar with the theoretical underpinnings of the model; cognitive computational modelling is inherently interdisciplinary. Understanding a theory or a model, learning (more about) programming (especially true for students), finding out what parts of a specification are intrinsic to a model's success (and thus should be part of the model itself!), discovering what parts of a theory are encapsulated in a specific model, etc. - all of these pedagogical aims are integral to grasping the internal mechanisms of a specific model, but also the whole of modelling as a scientific endeavour. How better to check one really understands how to program cognitive models than by [reimplementing a published working model](http://oliviaguest.com/doc/guest_14.pdf) and comparing the results of one's own implementation in terms of the parameters, the time and space complexity (if the code is made accessible by the original authors), the fit to the participant data, and so on? Such an exercise will not only serve to recapitulate what a student has learned in class but also to foment their desire to create their own models. This is in much the same vein as asking students to help out with a more senior experimenter's data collection from participants; doing something oneself is the best way of learning. Importantly though this is not about [getting students to do the "boring" work](http://www.nature.com/news/could-students-solve-the-irreproducibility-crisis-1.18095) that others eschew! A fresh pair of eyes will likely uncover new pros and cons and present new arguments for and against a model. To wit, sometimes old codebases need to be completely thrown out, and that is a good thing because they are often re-written by student scientists who will learn from the process and help the community.

[![](http://farm3.static.flickr.com/2001/2508770940_b88b955aaa.jpg)](http://farm3.static.flickr.com/2001/2508770940_b88b955aaa.jpg)
Knowing how to code is a [necessary](http://decafbad.com/blog/2012/05/16/please-learn-to-code/) but not sufficient requirement for understanding models.

4. _Can the model be extended?_
Extending models to account for more effects within the same theoretical framework, or to account for a broader range of participants, etc., will usually require a reimplementation for the reasons touched on above. A general problem in cognitive modelling, that [has been](http://ict.usc.edu/pubs/Towards%20functionally%20elegant%20grand%20unified%20architectures.pdf) [discussed](http://www.iccm2015.org/proceedings/papers/0063/paper0063.pdf) but largely not addressed by modellers, is that many computational accounts are not built upon, but merely used as a proof of concept and then all but abandoned. Even though a theory may comprise a number of useful predictions and [explanations](http://link.springer.com/article/10.1023/A:1022941621273), the models generated within the theory often do not manage to account for all the theory. In other words, successful models within a theory do not build on each other to account for more, but instead exist as parallel and independent instances of sub-theories within the theoretical framework; meaning that a successful model is created per prediction/explanation within a theory. This should not be the case if the theory is internally and externally consistent: a single model should be implementable. In many ways cognitive modelling is generally carried out contra to [Newell's](https://books.google.co.uk/books?hl=en&lr=&id=1lbY14DmV2cC&oi=fnd&pg=PA1&dq=Newell,+Allen.+1990.+Unified+Theories+of+Cognition.+Harvard+University+Press,+Cambridge,+Massachusetts.&ots=obPnZ_zWJ6&sig=5of9rnYzEMfE5pxHtsTx0Xyvc2w#v=onepage&q&f=false) [proposal](http://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2009.01063.x/full) for [unified theories of cognition](http://www.sciencedirect.com/science/article/pii/S1389041708000557). Parallels here are less obvious with non-computational work in psychology, especially since statistical hypothesis testing is about focusing on a very narrow question within a wider theory. But nonetheless, narrow hypotheses still have to be congruent and build upon the central theoretical account one is subscribing to.

[![](https://octodex.github.com/images/femalecodertocat.png)](https://octodex.github.com/images/femalecodertocat.png)
[Github](http://github.com/) facilitates sharing code, locating dependable [implementations of algorithms](https://github.com/showcases/machine-learning), reporting bugs, meeting other similar coders, even [hosting a journal](https://github.com/ReScience); and they [now](https://github.com/blog/1840-improving-github-for-science) cater specifically to [scientists' needs](https://github.com/showcases/science).

5. _Does another choice other than reimplementing even exist?_
Finally, unfortunately many models' code is not openly accessible. Often the original authors are either unwilling or unable to provide code because it has become totally unusable after a few years or already was spaghetti code to begin with (see question 2). Subsequently, other scientists are forced to have to write their own versions. This question might be moot in the near future because lots of excellent tools now exist that allow for both formally attributing work to individual coders, for collaborating with people within and between labs, and for allowing public access (e.g., [github.com](http://github.com/), [bitbucket.com](http://bitbucket.com/), [osf.io](http://osf.io/), [etc](http://connectedresearchers.com/online-tools-for-researchers/#dataandcode)). However, that was not the case even 5 years ago when a lot of these tools were nascent, non-existent, and/or unknown to the majority of the scientific community. Things have now changed, and hopefully older models will be reimplemented and/or uploaded and newer models will be developed using these tools. Making codebases public also helps others to accomplish all the previous points, i.e., add to, fix bugs in, and learn from the code.

### Conclusion

Making the code public will never remove the need for replication itself, as a replication requires the model is (re)implemented based on the published specification and not on reverse engineering the code. If one is forced to reverse engineer, e.g., C code to recreate the results in Python then the original authors have in part failed to communicate their model and theory. This is where analogues to empirical work become interesting and more complex. If a commonly accepted effect is nonetheless based on a confound, or [p-hacking](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106) ([which](http://www.nature.com/news/scientific-method-statistical-errors-1.14700) [some](http://blogs.discovermagazine.com/neuroskeptic/2015/05/18/p-hacking-a-talk-and-further-thoughts/#.Vgk96ZfsY_s) [see](https://peerj.com/preprints/1266/) as [neutral](http://fivethirtyeight.com/features/science-isnt-broken/) and some as [bad](http://io9.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800)) or some other explanation can better account for the phenomenon other than the accepted one, it is likely given enough time and interest this will be discovered without replications as others will often built upon it to design follow-up experiments. If the experiments that assume the irreplicable effect get none of the predicted results, it is likely it will call into question the whole theory, not just the original effect.

In modelling, it is less often the case that implementations are built upon in such a way (recall question 4), and even if they are it is usually not clear if the authors check the original findings are indeed present (replicated!) in their spin-off models. Evidence from modelling is less tied to the "real world" (because of modelling assumptions being completely under the control of the modeller, because of the [principle of multiple realisability](http://plato.stanford.edu/entries/multiple-realizability/), because models are created using [Turing complete](https://en.wikipedia.org/wiki/Turing_completeness) programming languages, etc.) and thus (perceived to be) less important to the theories models are generated from.

The point of replicating models is not only to determine if the original results replicate, but also to map out the space of simulations that the model specification allows, to discover important bugs, to augment models and theories, as well as to aid in the understanding of the implementation process and so on. While research in modelling should not by any means be paused in order to expend time and energy to replicate previously published work, replicating useful successful interesting models is completely unavoidable so modellers might as well get the most out of it!

### Further Reading

#### On This Blog

- [Meta-analyses of infant development: What's in it for computational modeling?](http://bootphon.blogspot.com/2015/06/meta-analyses-of-infant-development.html) by [Christina Bergmann](https://sites.google.com/site/chbergma/)

- [A fate worse than studying language acquisition](http://bootphon.blogspot.com/2015/08/a-fate-worse-than-studying-language.html) by [Ewan Dunbar](http://ewan.website/)

- [Evaluating models of language acquisition: are utility metrics useful?](http://bootphon.blogspot.com/2015/05/models-of-language-acquisition-machine.html) by [Emmanuel Dupoux](http://www.lscp.net/persons/dupoux/)

#### Journal Articles

- Addyman, C., & French, R. M. (2012). [Computational modeling in cognitive science: A manifesto for change.](http://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2012.01206.x/full) _Topics in cognitive science_, 4(3), 332-341.

- Byrne, M. D. (2013). [How many times should a stochastic model be run? An approach based on confidence intervals.](http://iccm-conference.org/2013-proceedings/papers/0083/paper0083.pdf) In _Proceedings of the 12th International conference on cognitive modeling_, Ottawa.

- Cooper, R. P., & Guest, O. (2014). [Implementations are not specifications: Specification, replication and experimentation in computational cognitive modeling.](http://www.sciencedirect.com/science/article/pii/S1389041713000314) _Cognitive Systems Research_, 27, 42-49.

- Fern√°ndez, J. (2003). [Explanation by computer simulation in cognitive science.](http://link.springer.com/article/10.1023/A:1022941621273) _Minds and Machines_, 13(2), 269-284.

- French, R. M., & Thomas, E. (2015). [Interactive effects of explicit emergent structure: A major challenge for cognitive computational modeling.](http://onlinelibrary.wiley.com/doi/10.1111/tops.12135/full) _Topics in cognitive science_, 7(2), 206-216.

- McClelland, J. L. (2009). [The place of modeling in cognitive science.](http://psych.stanford.edu/~jlm/papers/McClelland09PlaceOfModelingtopiCS.pdf) _Topics in Cognitive Science_, 1(1), 11-38.

- Pitt, M. A., Myung, I. J., & Zhang, S. (2002). [Toward a method of selecting among computational models of cognition.](http://smash.psych.nyu.edu/courses/spring09/modeling/materials/mdl.pdf) _Psychological review_, 109(3), 472.

- Roberts, S., & Pashler, H. (2000). [How persuasive is a good fit? A comment on theory testing.](http://www.ncbi.nlm.nih.gov/pubmed/10789200) _Psychological review_, 107(2), 358.

- Schellinck, J., & Webster, R. (2013). [Cognitive models: Understanding their critical role as explanatory and predictive hypothesis generators in cognition research.](http://iccm-conference.org/2013-proceedings/papers/0074/paper0074.pdf) In Proceedings of the _International Confrenece on Cognitive Modeling_ (ICCM-2013).

- Shiffrin, R. M., Lee, M. D., Kim, W., & Wagenmakers, E. J. (2008). [A survey of model evaluation approaches with a tutorial on hierarchical Bayesian methods.](http://onlinelibrary.wiley.com/doi/10.1080/03640210802414826/full) _Cognitive Science_, 32(8), 1248-1284.

- Topalidou, M., Leblois, A., Boraud, T., & Rougier, N. P. (2015). [A long journey into reproducible computational neuroscience.](http://journal.frontiersin.org/article/10.3389/fncom.2015.00030/full) _Frontiers in computational neuroscience_, 9.

_This post first appeared on [The Synthetic Learner Blog](http://bootphon.blogspot.fr/): [here](http://bootphon.blogspot.fr/)_
